
    
    
    
    
    [{"authors":null,"categories":null,"content":"I am a 3th-year PhD student of Artificial Intelligence at Gaoling School of Artificial Intelligence, Renmin University of China. I am advised by Prof. Ruihua Song. My research interests include computer vision, natural language processing (NLP), and Multi-modality.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a 3th-year PhD student of Artificial Intelligence at Gaoling School of Artificial Intelligence, Renmin University of China. I am advised by Prof. Ruihua Song. My research interests include computer vision, natural language processing (NLP), and Multi-modality.","tags":null,"title":"Yuchong Sun","type":"authors"},{"authors":["Hongwei Xue*","Yuchong Sun*","Bei Liu","Jianlong Fu","Ruihua Song","Houqiang Li","Jiebo Luo"],"categories":null,"content":"","date":1674172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1674172800,"objectID":"441dcdf499a7289f50e068c8bd1cb709","permalink":"https://ycsun1972.github.io/publication/clip-vip/","publishdate":"2023-01-20T00:00:00Z","relpermalink":"/publication/clip-vip/","section":"publication","summary":"We adapt image-text pre-trained models to video-text pre-training (i.e., post-pretraining). In this work, we propose an Omnisource Cross-modal Learning method equipped with a Video Proxy mechanism on the basis of CLIP, namely CLIP-ViP.","tags":[],"title":"CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment","type":"publication"},{"authors":["Yuchong Sun","Hongwei Xue","Ruihua Song","Bei Liu","Huan Yang","Jianlong Fu"],"categories":null,"content":"","date":1669593600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669593600,"objectID":"b3648f1b7b020383e311365125db28c6","permalink":"https://ycsun1972.github.io/publication/lfvila/","publishdate":"2022-11-28T00:00:00Z","relpermalink":"/publication/lfvila/","section":"publication","summary":"We introduce a Long-Form VIdeo-LAnguage pre-training model (LF-VILA) and train it on a large-scale long-form video and paragraph dataset constructed from HD-VILA-100M. We propose a Multimodal Temporal Contrastive (MTC) loss to learn the temporal relation across different modalities by encouraging fine-grained alignment between long-form videos and paragraphs. We then propose a Hierarchical Temporal Window Attention (HTWA) mechanism to effectively capture long-range dependency while reducing computational cost in Transformer.","tags":[],"title":"Long-Form Video-Language Pre-Training with Multimodal Temporal Contrastive Learning","type":"publication"},{"authors":["Xu Gu*","Yuchong Sun*","Feiyue Ni","Shizhe Chen","Ruihua Song","Boyuan Li","Xiang Cao"],"categories":null,"content":"","date":1669593600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669593600,"objectID":"3c2fa7f589d7665797652230efce8b5f","permalink":"https://ycsun1972.github.io/preprint/tevis/","publishdate":"2022-11-28T00:00:00Z","relpermalink":"/preprint/tevis/","section":"preprint","summary":"A storyboard is a roadmap for video creation which consists of shot-by-shot images to visualize key plots in a text synopsis. Creating video storyboards however remains challenging which not only requires association between high-level texts and images, but also demands for long-term reasoning to make transitions smooth across shots. In this paper, we propose a new task called Text synopsis to Video Storyboard (TeViS) which aims to retrieve an ordered sequence of images to visualize the text synopsis. We construct a MovieNet-TeViS benchmark based on the public MovieNet dataset. It contains 10K text synopses each paired with keyframes that are manually selected from corresponding movies by considering both relevance and cinematic coherence. We also present an encoder-decoder baseline for the task. The model uses a pretrained vision-and-language model to improve high-level text-image matching. To improve coherence in long-term shots, we further propose to pre-train the decoder on large-scale movie frames without text. Experimental results demonstrate that our proposed model significantly outperforms other models to create text-relevant and coherent storyboards. Nevertheless, there is still a large gap compared to human performance suggesting room for promising future work.","tags":[],"title":"Translating Text Synopses to Video Storyboards","type":"preprint"},{"authors":["Hongwei Xue*","Tiankai Hang*","Yanhong Zeng*","Yuchong Sun*","Bei Liu","Huan Yang","Jianlong Fu","Baining Guo"],"categories":null,"content":"","date":1654128000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654128000,"objectID":"e209fbdb7637ac86308f3eaf00c9b345","permalink":"https://ycsun1972.github.io/publication/hdvila/","publishdate":"2022-06-02T00:00:00Z","relpermalink":"/publication/hdvila/","section":"publication","summary":"We collect a large dataset which is the first high-resolution dataset including 371.5k hours of 720p videos and the most diversified dataset covering 15 popular YouTube categories. We propose a novel High-resolution and Diversified VIdeo-LAnguage pre-training model (HD-VILA) for many visual tasks.","tags":[],"title":"Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions","type":"publication"}]